{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43225676-a0ef-4f7a-b34c-e966e45a3b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T15:26:35.167271Z",
     "iopub.status.busy": "2025-03-15T15:26:35.166973Z",
     "iopub.status.idle": "2025-03-15T15:26:35.170797Z",
     "shell.execute_reply": "2025-03-15T15:26:35.170304Z",
     "shell.execute_reply.started": "2025-03-15T15:26:35.167255Z"
    }
   },
   "source": [
    "# 简单的 RAG 应用\n",
    "\n",
    "本节我们完成一个简单的 RAG 应用。我们将一个文档向量化后，存入向量数据库中，然后用 `deepseek-r1:1.5b` 模型，整合 RAG 取回的内容后输出回答。\n",
    "\n",
    "最终的 langflow 工作流如下：\n",
    "\n",
    "![](./img/simple_rag_app.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee2c96-283f-4bcb-99c1-5ac492807de1",
   "metadata": {},
   "source": [
    "## 1. 环境准备\n",
    "\n",
    "搭这个应用，最难的反而是搭环境这一步 (`ヮ´ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3907f5f-3368-4273-b617-a7b01985f9de",
   "metadata": {},
   "source": [
    "### 1.1 安装 Ollama\n",
    "\n",
    "1）下载 Ollama\n",
    "\n",
    "下载 Ollama：[ollama.com/download](https://ollama.com/download)\n",
    "\n",
    "它提供了 macOS, Linux, Windows 三种系统的下载方式。直接按你的系统安装就行。Windows 电脑建议直接安装，不用多此一举安装到 wsl。\n",
    "\n",
    "Linux 用户用以下命令安装：\n",
    "\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "查看版本号：\n",
    "\n",
    "```bash\n",
    "ollama --version\n",
    "```\n",
    "\n",
    "2）下载模型\n",
    "\n",
    "下载词向量模型 bge-m3 和推理模型 deepseek-r1:1.5b\n",
    "\n",
    "```bash\n",
    "# 下载 bge-m3\n",
    "ollama pull bge-m3\n",
    "\n",
    "# 下载 deepseek-r1:1.5b\n",
    "ollama pull deepseek-r1:1.5b\n",
    "```\n",
    "\n",
    "列出所有模型：\n",
    "\n",
    "```bash\n",
    "ollama list\n",
    "```\n",
    "\n",
    "3）启动 deepseek-r1\n",
    "\n",
    "```bash\n",
    "ollama run deepseek-r1:1.5b\n",
    "```\n",
    "\n",
    "启动后，你可以在命令行与 deepseek-r1 交互。\n",
    "\n",
    "PS: 更多命令我放在项目目录下的 [./deploy/install_ollama.sh](./deploy/install_ollama.sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3174c5b-fe8c-4405-8549-8cc6fdae5136",
   "metadata": {},
   "source": [
    "### 1.2 安装 langflow\n",
    "\n",
    "1）安装\n",
    "\n",
    "如果你是 Windows 电脑，建议把 langflow 安装到 wsl 的 Ubuntu 环境。因为 Windows 的 Python 环境管理实在是抽象。\n",
    "\n",
    "```bash\n",
    "# 创建 conda 环境\n",
    "conda create -n langflow python=3.11 -y\n",
    "\n",
    "# 激活 conda 环境\n",
    "conda activate langflow\n",
    "\n",
    "# 下载 langflow 通过阿里云镜像站\n",
    "pip install langflow -U -i https://mirrors.aliyun.com/pypi/simple/\n",
    "\n",
    "# 运行 langflow\n",
    "langflow run\n",
    "```\n",
    "\n",
    "安装 langflow 的过程会有点久。详情参考：[docs.langflow.org](https://docs.langflow.org/)\n",
    "\n",
    "2）快速启动\n",
    "\n",
    "我创建了一个 langflow 的快速启动脚本，用来快速切换到 langflow 的 Conda 环境并启动 langflow：\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "source $(conda info --base)/etc/profile.d/conda.sh\n",
    "conda activate langflow\n",
    "langflow run\n",
    "```\n",
    "\n",
    "程序文件见 [./langflow_start.sh](./langflow_start.sh)\n",
    "\n",
    "快速启动 langflow 在命令行运行：\n",
    "\n",
    "```bash\n",
    "bash langflow_start.sh\n",
    "# 或 ./langflow_start.sh\n",
    "```\n",
    "\n",
    "### 1.3 安装 chroma\n",
    "\n",
    "```bash\n",
    "conda activate langflow\n",
    "pip install chromadb -i https://mirrors.aliyun.com/pypi/simple/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab177660-10d3-47dc-ac0d-7d103bde8a25",
   "metadata": {},
   "source": [
    "## 2. langflow 搭建工作流\n",
    "\n",
    "启动 langflow 需要一点的时间：\n",
    "\n",
    "```bash\n",
    "# 如果是 Windows 电脑，确保先打开 wsl\n",
    "bash langflow_start.sh\n",
    "```\n",
    "\n",
    "浏览器打开 langflow 的默认端口 http://127.0.0.1:7860/ 进入 UI 界面。\n",
    "\n",
    "![](./img/langflow_start.jpg)\n",
    "\n",
    "1）创建一个新的 Flow\n",
    "\n",
    "点击 `+ New Flow` 选择 `Q&A` 分页，然后点击 `Vector Store RAG`\n",
    "\n",
    "![](./img/new_rag_flow.jpg)\n",
    "\n",
    "2）初始界面\n",
    "\n",
    "现在它提供给你的已经是一个完整的 RAG 工作流。可以看到这个工作流分上、下两块：\n",
    "\n",
    "- 上面是把用户问题向量化，然后到向量库取回相关文本，再使用提示词模板结合用户问题与相关文本，送入大模型的过程\n",
    "- 下面是把文本切分成文本片段，转成向量，然后存入向量数据库的过程\n",
    "\n",
    "![](./img/simple_rag_app_init.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097614d-67cc-4e97-9aef-cbd7aa8ea209",
   "metadata": {},
   "source": [
    "3）本地改造计划\n",
    "\n",
    "下面来改造它。它有三个地方要改：Embedding 方法要改成本地的 bge-m3；Astra DB 也要换成本地的 chroma；OpenAI 的模型也要换成本地的 deepseek-r1。\n",
    "\n",
    "4）启动 Ollama 服务\n",
    "\n",
    "因为 bge-m3 和 deepseek-r1 都是由 Ollama 支持的。因此要先启动 Ollama 服务.\n",
    "\n",
    "我是 Windows 电脑，Ollama 在本机，但是 langflow 在 wsl，如果在 `127.0.0.1` 启动 Ollama 服务，wsl 是访问不到的，必须在 `0.0.0.0` 启动才行。因此启动前需要配置环境变量：\n",
    "\n",
    "```bash\n",
    "# 对于 Windows PowerShell\n",
    "$env:OLLAMA_HOST = \"0.0.0.0:11434\"\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "对于 Linux 应该使用 `export OLLAMA_HOST=0.0.0.0:11434` 配置环境变量。\n",
    "\n",
    "> PS: 此时在浏览器访问 [http://127.0.0.1:11434/](http://127.0.0.1:11434/)，应该可以看到显示 `Ollama is running` 这几个字。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3e433-334c-48f9-8e30-10d5c27e37ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T17:22:00.535864Z",
     "iopub.status.busy": "2025-03-15T17:22:00.534139Z",
     "iopub.status.idle": "2025-03-15T17:22:00.540936Z",
     "shell.execute_reply": "2025-03-15T17:22:00.540411Z",
     "shell.execute_reply.started": "2025-03-15T17:22:00.535833Z"
    }
   },
   "source": [
    "5）添加 Ollama Embeddings 组件\n",
    "\n",
    "我们在侧边栏找到 `Ollama Embeddings` 这个组件，点击加号添加组件。\n",
    "\n",
    "![](./img/ollama_embed.jpg)\n",
    "\n",
    "然后我们在 `Ollama Base URL` 这里，在下拉栏中选择 `Add New Variable`，选择 Generic 页面，填 Name 和 value\n",
    "\n",
    "- **Name**: `ollama_url`\n",
    "- **Value**: `http://[YOUR_IP_HERE]:11434`\n",
    "\n",
    "> **[YOUR_IP_HERE] 的填法：**\n",
    "> \n",
    "> 1.如果你不是 Windows 电脑，也没有在 wsl 上打开 langflow，[YOUR_IP_HERE] 填 `127.0.0.1` 就好。\n",
    "> \n",
    "> 2.如果你是 Windows 电脑，并且在 wsl 打开 langflow，你需要获取本机 ip：\n",
    "> \n",
    "> ```bash\n",
    "> # Windows 电脑\n",
    "> ipconfig\n",
    "> ```\n",
    ">\n",
    "> 比如我的是 `192.168.2.192`，就在上面的 [YOUR_IP_HERE] 填上 `192.168.2.192`。\n",
    "\n",
    "填好后点击 Save Variable.\n",
    "\n",
    "![](./img/ollama_variable.jpg)\n",
    "\n",
    "然后在 Ollama Model 这里选 `bge-m3:latest`\n",
    "\n",
    "再复制一个 Ollama Embeddings 组件，把两个 OpenAI Embeddings 组件都替换掉。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc0ec2-354b-48bf-a903-ae841c31f60d",
   "metadata": {},
   "source": [
    "6）添加 Chroma DB 组件\n",
    "\n",
    "- **Collection Name**: `rag_simple_app`\n",
    "- **Persist Directory**: 新建一个 Generic 变量，Name 为 `chroma_path`，Value 为 `./chroma_db/`\n",
    "\n",
    "复制 Chroma DB 组件，替换掉两个 Astra DB。\n",
    "\n",
    "第一个 Chroma DB：\n",
    "\n",
    "![](./img/add_chroma_db_1.jpg)\n",
    "\n",
    "第二个 Chroma DB：\n",
    "\n",
    "![](./img/add_chroma_db_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e17a384-8e26-4155-b4db-6cca4a98d1ea",
   "metadata": {},
   "source": [
    "7）添加 Ollama 组件\n",
    "\n",
    "添加一个 Ollama 组件，替换掉 OpenAI 组件。\n",
    "\n",
    "Model Name 设为 deepseek-r1:1.5b，Temperature 参数设为 0.6\n",
    "\n",
    "![](./img/add_ollama.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b09ebb-36db-474c-8f75-41bc9883d123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T19:16:20.224173Z",
     "iopub.status.busy": "2025-03-15T19:16:20.223667Z",
     "iopub.status.idle": "2025-03-15T19:16:20.232898Z",
     "shell.execute_reply": "2025-03-15T19:16:20.232181Z",
     "shell.execute_reply.started": "2025-03-15T19:16:20.224121Z"
    }
   },
   "source": [
    "8）传入文档\n",
    "\n",
    "传入文档 [《人工智能伦理指南》](./docs/test_rag.md)，这是我让 qwen2.5-max 编的，用来测试 RAG 功能的文档。\n",
    "\n",
    "然后点击下方的 Chroma DB 的运行按钮 `▶`，这样就完成了文档的向量化，并将文本向量存入了 Chroma DB。\n",
    "\n",
    "再点开 Playground，我们在这里向 ChatBot 提问。\n",
    "\n",
    "![](./img/simple_rag_playground.jpg)\n",
    "\n",
    "可以看到，它准确地回答了我们编造的内容。\n",
    "\n",
    "由此可见，大模型真的能够读到我们上传的文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738adc0d-a4e6-493e-b23c-4340d84ba5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
